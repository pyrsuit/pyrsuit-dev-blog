[
    {
        "title": "Private 🚫 vs Protected 🛡️",
        "date": "2025-09-27",
        "text": "In Python, a single underscore means **private by convention**, while a double underscore triggers **name mangling** to protect attributes from being accidentally overridden in subclasses.\n\nBut what happens if a subclass defines its own double underscore attribute? How does a single-underscore variable differ from a double underscore variable in practice?\n\nFor example, the base `Metric` class has `__storage` and a `_mean`:\n\n```python\nclass Metric:\n    def __init__(self):\n        self.__storage = []  # double underscore → protected\n        self._mean = None  # single underscore → private\n\n    def add(self, num):\n        self.__storage.append(num)\n        self._mean = None\n\n    def mean(self):\n        if self._mean is None:\n            self._mean = sum(self.__storage) / len(self.__storage)\n        return self._mean\n\nm = Metric()\nm.add(5)\nm.add(8)\nprint(\"Metric mean:\", m.mean())  # 6.5\nprint(\"Metric __dict__:\", m.__dict__)  # {'_Metric__storage': [5, 8], '_mean': 6.5}\n```\n\nA subclass can define its own double underscore attribute without overwriting the parent:\n\n```python\nclass MaxMetric(Metric):\n    def __init__(self):\n        super().__init__()\n        self.__storage = None  # subclass storage, mangled separately\n\n    def add(self, num):\n        super().add(num)\n        if self.__storage is None or num > self.__storage:\n            self.__storage = num\n\n    def max(self):\n        return self.__storage\n\nmm = MaxMetric()\nmm.add(3)\nmm.add(10)\nmm.add(7)\nprint(\"MaxMetric mean:\", mm.mean())  # 6.666666666666667\nprint(\"MaxMetric max:\", mm.max())  # 10\nprint(\"MaxMetric __dict__:\", mm.__dict__)\n# {'_Metric__storage': [3, 10, 7], '_mean': 6.666666666666667, '_MaxMetric__storage': 10}\n```\n\n💡 **Takeaway**: Use **single** underscore for helpers or temporary state, **double** underscore for critical storage that shouldn't be accidentally overridden."
    },
    {
        "title": "Walrus operator: assign on the fly 🛫",
        "date": "2025-08-25",
        "text": "I recently discovered that Python has a neat operator for assigning a value and using it immediately: the `:=` operator, also called the walrus operator.\n\nFor example, instead of:\n\n```python\nvalue = data.get(\"key\")\nif value:\n    print(len(value))\n```\n\nYou can do:\n\n```python\nif (value := data.get(\"key\")):\n    print(len(value))\n```\n\nBoom 💥 value is assigned and ready to roll.\n\n💡 **Takeaway**: `:=` = assign inline, use instantly, keep it neat."
    },
    {
        "title": "The one when TRUNCATE locked me out 🚪🔒",
        "date": "2025-08-19",
        "text": "While running `pytest` with `MySQL`, I kept hitting tests stuck on: `Waiting for table metadata lock` 😫 \n\nDigging in, I found the culprit: my fixtures were using `TRUNCATE TABLE` to clean test data between runs. \n\nHere's the catch: In MySQL, `TRUNCATE` is __[DDL](https://en.wikipedia.org/wiki/Data_definition_language)__, not __[DML](https://en.wikipedia.org/wiki/Data_manipulation_language)__. It drops/recreates the table under the hood, resets `AUTO_INCREMENT`, and requires an **exclusive metadata lock**. If any transaction has touched the table — even just a `SELECT` — `TRUNCATE` will block until that lock is released. In `pytest`, with long-lived connections, this happened constantly. \n\n✅ **Fix:** switched to `DELETE FROM` table for cleanup. `DELETE` is `DML`, transactional, and only takes row locks + short metadata locks. It doesn't reset `AUTO_INCREMENT`, but it doesn't block other transactions either. \n\n💡 **Takeaway:**: In `MySQL` tests, prefer `DELETE` over TRUNCATE unless you can guarantee no open transactions. \n\n🔄 **Postgres comparison**: `TRUNCATE` in `Postgres` is transactional — you can roll it back, and it doesn't block in the same way. It still takes stricter locks than `DELETE`, but because `Postgres` metadata locking is less rigid, it rarely causes the same “hung DDL” issues you see in `MySQL`."
        
    },
    {
        "title": "When metrics eat your memory 🧠 🍽️",
        "date": "2025-07-24",
        "text": "Follow-up to my earlier post on Prometheus + Multiprocess Apps ... \n\nA few days in, I noticed the metrics directory was ballooning in memory 🎈 \n\nDigging in, I realized the Prometheus Python client (in multiprocess mode) writes separate files per metric per process. By default, those files are named things like `counter_12345.db`, where `12345` is the PID.\n\nSo when a `uWSGI` worker dies and gets replaced — totally normal behavior — the new process gets its own set of metric files. But the old files? They just stay there.\n\nSince the client doesn’t automatically clean up stale files, the directory just keeps growing.\n\n✅ Fix: I configured a cleanup step to remove metrics for dead processes.\n\n💡 Takeaway: In multiprocess mode, the metrics client tracks data per PID. Without cleanup, these files accumulate and quietly consume memory — especially in environments with frequent process restarts."
    },

    {
        "title": "Goodbye temp venv hacks 👋",
        "date": "2025-07-19",
        "text": "Today I learned how much I enjoy using `uv` scripts for quick, one-off tasks.\n\nYou can define dependencies right at the top of the script, and when you run it with `uv`, it spins up a temporary virtual environment automatically. Once the script finishes, the environment is destroyed — super clean 🧹\n\nThis is perfect for things like initial tasks when starting a container, or scripts that import data, run a migration, or do any kind of setup that isn't needed once the main app is running.\n\n💡 **Takeaway**: __[uv scripts](https://docs.astral.sh/uv/guides/scripts/)__ give you a disposable, isolated environment without any manual setup — ideal for clean, repeatable scripting without leaving a mess behind."
    },
    {
        "title": "Prometheus + multiprocess apps: A lesson from the trenches",
        "date": "2025-07-13",
        "text": "I recently deployed an API using `uWSGI` with multiple workers. I exposed a `/metrics` endpoint for `Prometheus` scraping — all looked good.\n\nUntil I realized… the metrics were off 🫠\n\nTurns out, when you're using multiple `uWSGI` workers, `Prometheus`' Python client needs **multiprocess mode** enabled to aggregate metrics across all worker processes. Without it, each process exposes its own separate metrics — so counters, for example, appear to jump up and down instead of increasing cumulatively across all workers. \n\n✅ **Fix:** Configured __[multiprocess mode](https://prometheus.github.io/client_python/multiprocess/)__, so all workers write metrics to a shared directory.\n\n💡 **Takeaway**: With multiple workers per replica, `Prometheus` scrapes the `/metrics` endpoint from only one worker per replica at random — so without multiprocess mode, your `Prometheus` metrics won't reflect the true state of your API — making it impossible to accurately track what's really happening."
    }
]