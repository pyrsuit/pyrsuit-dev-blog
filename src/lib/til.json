[
    {
        "title": "Private ğŸš« vs Protected ğŸ›¡ï¸",
        "date": "2025-09-27",
        "text": "In Python, a single underscore means **private by convention**, while a double underscore triggers **name mangling** to protect attributes from being accidentally overridden in subclasses.\n\nBut what happens if a subclass defines its own double underscore attribute? How does a single-underscore variable differ from a double underscore variable in practice?\n\nFor example, the base `Metric` class has `__storage` and a `_mean`:\n\n```python\nclass Metric:\n    def __init__(self):\n        self.__storage = []  # double underscore â†’ protected\n        self._mean = None  # single underscore â†’ private\n\n    def add(self, num):\n        self.__storage.append(num)\n        self._mean = None\n\n    def mean(self):\n        if self._mean is None:\n            self._mean = sum(self.__storage) / len(self.__storage)\n        return self._mean\n\nm = Metric()\nm.add(5)\nm.add(8)\nprint(\"Metric mean:\", m.mean())  # 6.5\nprint(\"Metric __dict__:\", m.__dict__)  # {'_Metric__storage': [5, 8], '_mean': 6.5}\n```\n\nA subclass can define its own double underscore attribute without overwriting the parent:\n\n```python\nclass MaxMetric(Metric):\n    def __init__(self):\n        super().__init__()\n        self.__storage = None  # subclass storage, mangled separately\n\n    def add(self, num):\n        super().add(num)\n        if self.__storage is None or num > self.__storage:\n            self.__storage = num\n\n    def max(self):\n        return self.__storage\n\nmm = MaxMetric()\nmm.add(3)\nmm.add(10)\nmm.add(7)\nprint(\"MaxMetric mean:\", mm.mean())  # 6.666666666666667\nprint(\"MaxMetric max:\", mm.max())  # 10\nprint(\"MaxMetric __dict__:\", mm.__dict__)\n# {'_Metric__storage': [3, 10, 7], '_mean': 6.666666666666667, '_MaxMetric__storage': 10}\n```\n\nğŸ’¡ **Takeaway**: Use **single** underscore for helpers or temporary state, **double** underscore for critical storage that shouldn't be accidentally overridden."
    },
    {
        "title": "Walrus operator: assign on the fly ğŸ›«",
        "date": "2025-08-25",
        "text": "I recently discovered that Python has a neat operator for assigning a value and using it immediately: the `:=` operator, also called the walrus operator.\n\nFor example, instead of:\n\n```python\nvalue = data.get(\"key\")\nif value:\n    print(len(value))\n```\n\nYou can do:\n\n```python\nif (value := data.get(\"key\")):\n    print(len(value))\n```\n\nBoom ğŸ’¥ value is assigned and ready to roll.\n\nğŸ’¡ **Takeaway**: `:=` = assign inline, use instantly, keep it neat."
    },
    {
        "title": "The one when TRUNCATE locked me out ğŸšªğŸ”’",
        "date": "2025-08-19",
        "text": "While running `pytest` with `MySQL`, I kept hitting tests stuck on: `Waiting for table metadata lock` ğŸ˜« \n\nDigging in, I found the culprit: my fixtures were using `TRUNCATE TABLE` to clean test data between runs. \n\nHere's the catch: In MySQL, `TRUNCATE` is __[DDL](https://en.wikipedia.org/wiki/Data_definition_language)__, not __[DML](https://en.wikipedia.org/wiki/Data_manipulation_language)__. It drops/recreates the table under the hood, resets `AUTO_INCREMENT`, and requires an **exclusive metadata lock**. If any transaction has touched the table â€” even just a `SELECT` â€” `TRUNCATE` will block until that lock is released. In `pytest`, with long-lived connections, this happened constantly. \n\nâœ… **Fix:** switched to `DELETE FROM` table for cleanup. `DELETE` is `DML`, transactional, and only takes row locks + short metadata locks. It doesn't reset `AUTO_INCREMENT`, but it doesn't block other transactions either. \n\nğŸ’¡ **Takeaway:**: In `MySQL` tests, prefer `DELETE` over TRUNCATE unless you can guarantee no open transactions. \n\nğŸ”„ **Postgres comparison**: `TRUNCATE` in `Postgres` is transactional â€” you can roll it back, and it doesn't block in the same way. It still takes stricter locks than `DELETE`, but because `Postgres` metadata locking is less rigid, it rarely causes the same â€œhung DDLâ€ issues you see in `MySQL`."
        
    },
    {
        "title": "When metrics eat your memory ğŸ§  ğŸ½ï¸",
        "date": "2025-07-24",
        "text": "Follow-up to my earlier post on Prometheus + Multiprocess Apps ... \n\nA few days in, I noticed the metrics directory was ballooning in memory ğŸˆ \n\nDigging in, I realized the Prometheus Python client (in multiprocess mode) writes separate files per metric per process. By default, those files are named things like `counter_12345.db`, where `12345` is the PID.\n\nSo when a `uWSGI` worker dies and gets replaced â€” totally normal behavior â€” the new process gets its own set of metric files. But the old files? They just stay there.\n\nSince the client doesnâ€™t automatically clean up stale files, the directory just keeps growing.\n\nâœ… Fix: I configured a cleanup step to remove metrics for dead processes.\n\nğŸ’¡ Takeaway: In multiprocess mode, the metrics client tracks data per PID. Without cleanup, these files accumulate and quietly consume memory â€” especially in environments with frequent process restarts."
    },

    {
        "title": "Goodbye temp venv hacks ğŸ‘‹",
        "date": "2025-07-19",
        "text": "Today I learned how much I enjoy using `uv` scripts for quick, one-off tasks.\n\nYou can define dependencies right at the top of the script, and when you run it with `uv`, it spins up a temporary virtual environment automatically. Once the script finishes, the environment is destroyed â€” super clean ğŸ§¹\n\nThis is perfect for things like initial tasks when starting a container, or scripts that import data, run a migration, or do any kind of setup that isn't needed once the main app is running.\n\nğŸ’¡ **Takeaway**: __[uv scripts](https://docs.astral.sh/uv/guides/scripts/)__ give you a disposable, isolated environment without any manual setup â€” ideal for clean, repeatable scripting without leaving a mess behind."
    },
    {
        "title": "Prometheus + multiprocess apps: A lesson from the trenches",
        "date": "2025-07-13",
        "text": "I recently deployed an API using `uWSGI` with multiple workers. I exposed a `/metrics` endpoint for `Prometheus` scraping â€” all looked good.\n\nUntil I realizedâ€¦ the metrics were off ğŸ« \n\nTurns out, when you're using multiple `uWSGI` workers, `Prometheus`' Python client needs **multiprocess mode** enabled to aggregate metrics across all worker processes. Without it, each process exposes its own separate metrics â€” so counters, for example, appear to jump up and down instead of increasing cumulatively across all workers. \n\nâœ… **Fix:** Configured __[multiprocess mode](https://prometheus.github.io/client_python/multiprocess/)__, so all workers write metrics to a shared directory.\n\nğŸ’¡ **Takeaway**: With multiple workers per replica, `Prometheus` scrapes the `/metrics` endpoint from only one worker per replica at random â€” so without multiprocess mode, your `Prometheus` metrics won't reflect the true state of your API â€” making it impossible to accurately track what's really happening."
    }
]